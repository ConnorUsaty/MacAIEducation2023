{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Let's implement GPT2 using only Numpy!"
      ],
      "metadata": {
        "id": "-2QcUdnzYnOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clone the repository containing external utility code"
      ],
      "metadata": {
        "id": "Y-mElWLSYtVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jaymody/picoGPT.git temp_dir\n",
        "!mv temp_dir/* .\n",
        "!rm -rf temp_dir\n",
        "!pip install fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW9mO69iNwa8",
        "outputId": "23ef04a0-78c1-4517-aa89-da020d50310c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'temp_dir'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 56 (delta 26), reused 14 (delta 14), pack-reused 23\u001b[K\n",
            "Receiving objects: 100% (56/56), 17.79 KiB | 1.98 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n",
            "Collecting fire\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=b7f6c9eb2f450ebe8efff14b65708d12c72c023ef840cbaee11b326d471ef0e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GELU activation function for non-linearity in neural networks."
      ],
      "metadata": {
        "id": "Ms33g10YZbha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))"
      ],
      "metadata": {
        "id": "bSVTPW5uZerO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax function for converting logits to probabilities."
      ],
      "metadata": {
        "id": "XirvZ2GyZffh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "pw1l6naHZkRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizes the input 'x' to have 0 mean and unit variance, then scales and shifts it."
      ],
      "metadata": {
        "id": "_6FwpJwjZlwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.var(x, axis=-1, keepdims=True)\n",
        "    x = (x - mean) / np.sqrt(variance + eps)\n",
        "    return g * x + b"
      ],
      "metadata": {
        "id": "CeaTgUS_ZnoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear transformation (matrix multiplication and bias addition)."
      ],
      "metadata": {
        "id": "zZ_jwh7jZoht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x, w, b):\n",
        "    return x @ w + b"
      ],
      "metadata": {
        "id": "JhOSeMh_Zq6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A two-layer feed-forward network with GELU non-linearity."
      ],
      "metadata": {
        "id": "CRr3l5SlZsVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ffn(x, c_fc, c_proj):\n",
        "    a = gelu(linear(x, **c_fc))\n",
        "    x = linear(a, **c_proj)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9hzpKwJPZuKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computes scaled dot-product attention with optional masking."
      ],
      "metadata": {
        "id": "eiXcts5dZwA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention(q, k, v, mask):\n",
        "    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v"
      ],
      "metadata": {
        "id": "utHaA5ldZxa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splits inputs for multi-head attention, applies attention, and recombines outputs."
      ],
      "metadata": {
        "id": "T1QEUIFWZ8Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mha(x, c_attn, c_proj, n_head):\n",
        "    x = linear(x, **c_attn)\n",
        "    qkv = np.split(x, 3, axis=-1)\n",
        "    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), qkv))\n",
        "    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n",
        "    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n",
        "    x = np.hstack(out_heads)\n",
        "    x = linear(x, **c_proj)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KsRLNVDiZ9xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applies a transformer block with multi-head attention and a feed-forward network."
      ],
      "metadata": {
        "id": "7A_9Z8GRZ-3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n",
        "    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oGvg9Cu4aA4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructs the GPT-2 model from inputs through multiple transformer blocks."
      ],
      "metadata": {
        "id": "yj9sg7qOaCqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n",
        "    x = wte[inputs] + wpe[range(len(inputs))]\n",
        "    for block in blocks:\n",
        "        x = transformer_block(x, **block, n_head=n_head)\n",
        "    x = layer_norm(x, **ln_f)\n",
        "    return x @ wte.T"
      ],
      "metadata": {
        "id": "nqGevZdeaEhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generates text by repeatedly applying the model and selecting the highest probability token."
      ],
      "metadata": {
        "id": "cgVhoy1xaG5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(inputs, params, n_head, n_tokens_to_generate):\n",
        "    for _ in range(n_tokens_to_generate):\n",
        "        logits = gpt2(inputs, **params, n_head=n_head)\n",
        "        next_id = np.argmax(logits[-1])\n",
        "        inputs.append(int(next_id))\n",
        "    return inputs[len(inputs) - n_tokens_to_generate:]"
      ],
      "metadata": {
        "id": "AOVYka0AaI7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The main function in the script will:\n",
        "\n",
        "- Load model parameters (GPT2 Weights)\n",
        "- Encode the input\n",
        "- Generate text\n",
        "- Decode the output."
      ],
      "metadata": {
        "id": "vWNpIAolaKW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try it yourself below!\n",
        "\n",
        "It will take some time running it for the first time since it has to download GPT2's weights that were originally trained and provided from OpenAI"
      ],
      "metadata": {
        "id": "uVWTI7UohfFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gpt2.py \"Hello my friend I am going to\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03dHe9OdfTCV",
        "outputId": "47fcce90-c9c1-418d-c85f-10231601bf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-02 13:58:14.581714: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-02 13:58:14.581764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-02 13:58:14.583739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-02 13:58:14.594902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-02 13:58:16.765239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Fetching checkpoint: 1.00kb [00:00, 5.27Mb/s]                                                       \n",
            "Fetching encoder.json: 1.04Mb [00:00, 4.54Mb/s]                                                     \n",
            "Fetching hparams.json: 1.00kb [00:00, 6.93Mb/s]                                                     \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mb [00:09, 54.2Mb/s]                                    \n",
            "Fetching model.ckpt.index: 6.00kb [00:00, 24.4Mb/s]                                                 \n",
            "Fetching model.ckpt.meta: 472kb [00:00, 3.51Mb/s]                                                   \n",
            "Fetching vocab.bpe: 457kb [00:00, 3.02Mb/s]                                                         \n",
            "generating: 100% 40/40 [00:14<00:00,  2.68it/s]\n",
            " be a little bit more careful about my hair. I am going to be a little bit more careful about my hair. I am going to be a little bit more careful about my hair. I am\n"
          ]
        }
      ]
    }
  ]
}